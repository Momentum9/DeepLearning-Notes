{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用PyTorch从零实现Softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 28, 28])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))[0].shape # 生成一个批 256个样本的特征 [1]为label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 初始化参数w和b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于softmax，需要将(1,28,28)的三维拉长为1*28*28的向量\n",
    "num_inputs = 784\n",
    "num_outputs = 10 # 数据共有十个类别\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) # (784, 10)\n",
    "b = torch.zeros(num_outputs, requires_grad=True) # 每个输出都要有个偏移 (10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 定义softmax函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z): # Z是个矩阵 (-1,10)\n",
    "    Z_exp = torch.exp(Z)\n",
    "    partition = Z_exp.sum(1, keepdim=True) # 因为共10列/类 故按行求和  并keepdim使能够下一行的广播\n",
    "    return Z_exp / partition # 运用了广播机制，让每一行都除以分母 (batch_size, 784)与(batch_size,1)进行广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试\n",
    "# X = torch.normal(0,1,(2,5))\n",
    "# x_prob = softmax(X)\n",
    "# x_prob,x_prob.sum(1) # 根据输出可见，所有输出值都为正数，且行和均为1（即各个类别的概率合为1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 定义softmax回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxRegressionNet(X): # 为何不把W和b也设为参数呢？\n",
    "    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b) # X本来为(256,1,28,28) ?\n",
    "    # reshape将每张图片的(1,28,28)展平为向量\n",
    "    # X(256,784) * W(784,10) + b(10) -> (256,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 介绍一下花式索引\n",
    "# a = torch.arange(12).reshape(3,4)\n",
    "# a[[0,2], [2,3]] # 拿出第1行第3列 和 第3行第4类元素\n",
    "\n",
    "y = torch.tensor([0,2])\n",
    "y_hat = torch.tensor([[0.1,0.3,0.6], [0.3,0.2,0.5]]) # 输出的各样本各类别的概率\n",
    "y_hat[[0,1], y] # 拿出第1行第1列 和 第2行第3类元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 定义交叉熵损失函数 <br>\n",
    "   交叉熵采用真实标签的预测概率的负对数似然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y): # y_hat(256, 10), y(256, ) 对吗?\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y]) # 拿出对应真实标号的预测值\n",
    "\n",
    "cross_entropy(y_hat, y) # y_hat(2,3) y(2,)  # 测试一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 将预测类别与真实y进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_hat, y): # 计算预测正确的数量\n",
    "    # 如果y_hat是矩阵，那么假定第二个维度存储每个类的预测分数\n",
    "    # 使用argmax获得每行中最大元素的索引来获得预测类别\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: # len(y_hat.shape)>1 说明是不止一维; and后面说明列数>1\n",
    "        y_hat = y_hat.argmax(axis=1) \n",
    "    cmp = y_hat.type(y.dtype) == y # 现将y_hat的类型转为y的类型以防类型不一致，然后与y比较生成bool列表\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "accuracy(y_hat, y) / len(y) # 正确预测数量 / 总预测数量 = 正确概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个使用程序类Accumulator,用于对多个变量进行累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:  #@save\n",
    "    \"\"\"在n个变量上累加\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估数据迭代器data_iter在任意模型net的精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0745"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    \"\"\"计算在指定数据集data_iter上模型net的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module): # 如果模型是torch nn实现的，转成评估模式(不计算梯度)\n",
    "        net.eval()\n",
    "    metric = Accumulator(2) # 累加器; 实例化\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X), y), y.numel()) # 正确数量，总数量\n",
    "    return metric[0] / metric[1] # 返回精度\n",
    "\n",
    "evaluate_accuracy(softmaxRegressionNet, test_iter)\n",
    "# 由于softmaxregressionNet模型使用随机的参数w和b初始化的，还未学习，接近随机猜测。那么10个类别随机猜测概率接近0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Softmax回归的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    if isinstance(net, torch.nn.Module):  # 如果是nnModule(内置训练模型)\n",
    "        net.train()\n",
    "    metric = Accumulator(3)  # 长度为3的迭代器 累加所需信息\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)  # 交叉熵损失\n",
    "        if isinstance(updater, torch.optim.Optimizer):  # 如果updater是内置的优化器\n",
    "            updater.zero_grad()\n",
    "            l.backward()  # 计算梯度\n",
    "            updater.step()  # 更新参数\n",
    "            metric.add(float(l)*len(y), accuracy(y_hat, y), y.size().numel())\n",
    "        else: # 自己实现的\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "615f2bc64d834d96908d844a686e951aa7993850d1defc534e0755cfeae37339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
