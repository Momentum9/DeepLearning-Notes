{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用PyTorch从零实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 人造数据<br>\n",
    "构造一个人造数据集，w=[2,-3.4], b=4.2, 带有噪声$\\epsilon$ 即 :\n",
    "$$\n",
    "y = Xw+b+\\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples): # 人造数据集\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) # 均值为0方差为1的服从正态分布的随机数;\n",
    "    # 尺寸为 num_examples行 len(w)列; (1000,2)\n",
    "    y = torch.matmul(X,w) + b # (1000,2) * (2,1) + (1)\n",
    "    y += torch.normal(0, 0.01, y.shape) # 添加噪声\n",
    "\n",
    "    return X, y.reshape((-1,1)) # 将y做成一个列向量(1000,1)返回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "# features.shape (1000,2) labels.shape(1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 可视化一下\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter(features[:,1].numpy(), labels.numpy(), s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义一个生成小批量的函数<br>\n",
    "data_iter输入为批量大小、特征矩阵和标签向量，迭代生成大小为batch_size的小批量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.6238, -1.1909],\n",
       "         [-1.3182, -0.1359],\n",
       "         [-0.1220,  0.5916],\n",
       "         [-0.0552, -0.2519],\n",
       "         [-0.7003, -0.3164],\n",
       "         [ 0.5688, -0.0320],\n",
       "         [ 1.3282,  0.0398],\n",
       "         [-0.6529, -0.5951],\n",
       "         [-0.2985, -0.4908],\n",
       "         [-0.4694, -0.5937]]),\n",
       " tensor([[3.0169],\n",
       "         [2.0194],\n",
       "         [1.9531],\n",
       "         [4.9360],\n",
       "         [3.8844],\n",
       "         [5.4443],\n",
       "         [6.7189],\n",
       "         [4.9108],\n",
       "         [5.2679],\n",
       "         [5.2893]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_iter(batch_size, features, labels): # generator function\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices) # 随机打乱下标\n",
    "    # batch_features = features[indices] # 这样不行，因为目的是每次生成一个batch_size 而非num_examples个\n",
    "    for i in range(0, num_examples, batch_size): # 步长为batch_size 妙\n",
    "        batch_indices = torch.tensor(indices[i:min(i+batch_size, num_examples)]) # 细节：min是为了防止下标超出上界\n",
    "        # 生成[0:10],[10,20],...,[990,1000]  但是此处是从经过shuffle的indices中索引，故得到一个batchsize的随机样本\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "        # yield关键字将函数变成generator function生成函数\n",
    "        \n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# # 通过for循环得到生成器内容x\n",
    "# for X, y in data_iter(batch_size, features, labels):\n",
    "#     print(X,'\\n',y) \n",
    "#     break # 这儿就输出一个batch的样本\n",
    "g = data_iter(batch_size, features, labels)\n",
    "next(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 定义初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0,0.01,size=(2,1), requires_grad=True) # 初始化为正态分布 需要梯度\n",
    "b = torch.zeros(1, requires_grad=True) # 初始化为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X,w) + b # 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y): # 此函数未求平均 mean\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 # 保险起见，因为二者可能一个为行向量一个列向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size): # 小批量随机梯度下降\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        params (list): 包含所有参数w,b的一个list\n",
    "        lr (_type_): learning rate\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): # 此模块内的变量不进行自动求导 requires_grad = False (更新时不需要计算梯度)\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size # 因为此处是对cost求偏导；square_loss中没除N\n",
    "            param.grad.zero_() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 0.033335\n",
      "epoch 2, train_loss 0.000123\n",
      "epoch 3, train_loss 0.000055\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels): # 1000个样本被随机分为了100个batch\n",
    "        # Forward\n",
    "        y_pred = net(X,w,b)\n",
    "        l = loss(y_pred, y) # X和y的小批量损失; shape=(batch_size,1)\n",
    "        \n",
    "        # Backward & Updata\n",
    "        # 由于l不是标量scaler 求导需.sum()\n",
    "        l.sum().backward()\n",
    "        sgd([w,b], lr, batch_size)\n",
    "    # 进行了一个epoch之后 观察评价一下进度\n",
    "    with torch.no_grad(): # 无需计算梯度\n",
    "        train_l = loss(net(features,w,b), labels) # 训练集损失；此处是1000个样本\n",
    "        print(f'epoch {epoch+1}, train_loss {train_l.mean():f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差：tensor([ 0.0001, -0.0002], grad_fn=<SubBackward0>)\n",
      "b的估计误差：tensor([5.6267e-05], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# 比较真是参数和训练学到的参数来评估训练的成功程度\n",
    "print(f'w的估计误差：{true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差：{true_b - b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "615f2bc64d834d96908d844a686e951aa7993850d1defc534e0755cfeae37339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
