{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过深度学习框架简洁地实现线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data \n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.创建数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000) # 生成数据\n",
    "# features.shape (1000,2)  labels.shape (1000,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.利用TensorDataset和DataLoader自动创建生成小批量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.6334,  0.0691],\n",
       "         [ 1.4405, -0.8137],\n",
       "         [-1.3261,  0.8369],\n",
       "         [ 0.1328,  1.6695],\n",
       "         [-0.7945,  0.4001],\n",
       "         [-0.3057,  1.3405],\n",
       "         [-0.9769, -0.6522],\n",
       "         [-0.2217,  0.6948],\n",
       "         [ 1.2154,  2.3783],\n",
       "         [ 0.4641, -0.3715]]),\n",
       " tensor([[ 0.6994],\n",
       "         [ 9.8418],\n",
       "         [-1.3081],\n",
       "         [-1.2115],\n",
       "         [ 1.2565],\n",
       "         [-0.9617],\n",
       "         [ 4.4657],\n",
       "         [ 1.4059],\n",
       "         [-1.4456],\n",
       "         [ 6.3758]])]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_array(features, labels, batch_size, is_train=True):\n",
    "    # 构造一个PyTorch数据迭代器\n",
    "    dataset = data.TensorDataset(features, labels)\n",
    "    # 直接用 TensorDataset() 将tensor类型的数据x_data和y_data包装成Dataset类，便于后续作为DataLoader的参数\n",
    "\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train) # 每次从中随机挑选batch_size个样本\n",
    "    # 只对训练集进行shuffle 测试集不shuffle\n",
    "    # TensorDataset将输入的两类数据一一对应；DataLoader将两类数据重新排序\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array(features, labels, batch_size) # 此处直接调用torch.utils中的data库，生成类似迭代器\n",
    "# type(data_iter) # torch.utils.data.dataloader.DataLoader\n",
    "next(iter(data_iter)) # 转换为Python的可迭代对象; 并输出一个; 也可用for循环输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn # neural network 定义了大量的层\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2,1)) # 用Linear层(线性层/全连接层)；输入维度2 输出维度1\n",
    "# 然后将Linear层放到Sequential容器中，该容器理解为List of layers，将层按序放在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.6502, 0.4347]], requires_grad=True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight # 会被自动默认初始化为服从均值分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 定义初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net[0] # net[0]=Linear(in_features=2, out_features=1, bias=True)\n",
    "# net[0].bias是张量b，net[0].weight是张量w\n",
    "net[0].weight.data.normal_(0,0.01) # 初始化\n",
    "net[0].bias.data.fill_(0) # 偏差设为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 使用内置的均方误差MSELoss和SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.03) # net.parameters()是个generator包含所有的参数w和b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 0.000271\n",
      "epoch 2, train_loss 0.000107\n",
      "epoch 3, train_loss 0.000107\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        # Forward\n",
    "        y_pred = net(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        # Backward\n",
    "        optimizer.zero_grad() # 先清零优化器的梯度\n",
    "        loss.backward() # pytorch自动进行了sum\n",
    "        # Updata\n",
    "        optimizer.step() # 更新模型；优雅\n",
    "    loss = criterion(net(features), labels)\n",
    "    print(f'epoch {epoch+1}, train_loss {loss:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测后的参数w:  tensor([[ 1.9998, -3.3996]])\n",
      "预测后的参数b:  tensor([4.2001])\n"
     ]
    }
   ],
   "source": [
    "print(f'预测后的参数w: ', net[0].weight.data)\n",
    "print(f'预测后的参数b: ', net[0].bias.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "615f2bc64d834d96908d844a686e951aa7993850d1defc534e0755cfeae37339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
