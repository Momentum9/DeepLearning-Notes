# Record

## LeNet

单GPU 2080Ti

![图 1](images/2eda5d7762836c1aaf794ca20785b24b2ecf14a5d2199a78d0beca22650d70ad.png)  

![图 4](images/e6efb4b4f081a64107215a7f02da6f8e5d5f86f5c9111fb7e574f516ff52d994.png)  

![图 11](images/adff8087dfc0b86c895feb87c0cea4284714ec0a790e1591bbd23eb0ff4a4083.png)  

在epoch一致时，学习率从0.1-10递增变化的影响观察如下：

- 时间上：一致
- examples/sec：基本在70000
- loss：从0.1递增到2.5都是一直递减，然后缓慢上升最后直接封顶
- 训练集精度：从0.1-3基本是递增的，然后降低然后为0
- 测试集精度类似（未能观测出过拟合问题）

> 开始loss递减原因：学习率更大，收敛了更多
> 然后loss递增原因：学习率过大，逐渐无法收敛，步伐太大，直接跳过了最优值。

![图 14](images/096dda3e07ce818c70194f5d628c495a54a0ab9c598e11465900a474b1150d6c.png)  

![图 15](images/5566d0ebd67d555b1c14a8b2a0acba2f553e6a80da3b815e8e632e55fd21909d.png)  

在固定epoch的时候，学习率比较小时，误差比较大是因为模型还未收敛，需要更多的epoch才行，下图证实：
![图 17](images/3133d85bba296e7e8391c39745e72d6b0331987c51bef8480367a14b33b82818.png)  
（与上文0.1对比，其loss低很多）

**故学习率较小时模型收敛缓慢，需要更多次的epoch**
如果学习率过小，那将看起来像是平行线，基本不收敛。
![图 16](images/cde7989f570ada701b396c17218b225f918bca8b03998d7c6898c8c858e00d33.png)  

**总结：**

- 学习率设置太小，网络收敛非常缓慢，需要更多epoch。
- 学习率设置太小，可能会进入局部极值点就收敛，没有真正找到的最优解，换句话说就是它步长太小，跨不出这个坑。
- 学习率设置太大，可能会不收敛  

在学习率一致时，epoch从5-100变化的影响：

- epoch过少时欠拟合
- 随着epoch的提升，loss持续下降直到平缓，训练集准确率和测试集准确率都持续上升，差距应该是会逐渐变大，发生过拟合（但此处不明显）
![图 18](images/5280634d850acc10c5aefaeb6308f19c56837ce08db32dc01c2460aec13c72ac.png)  
![图 19](images/3dfdb6e380b7cb638f77b972432c7f1a34444c38171b636e9c9af99fefcacbc8.png)  

## AlexNet

LeNet
![图 4](images/e6efb4b4f081a64107215a7f02da6f8e5d5f86f5c9111fb7e574f516ff52d994.png)  
AlexNet
![图 20](images/e37b46f7280158c33177b9d4b0a088fd7917ac64ee085354683eded0df86486f.png)  
> examples/sec锐减
> loss更低，准确率更高

VGGNet
![图 22](images/762d2945502a4591c7c90c61b14628d67616885871b381e29531d8423d64e7d5.png)  
> 通道数减少4倍（计算量降低16倍）后仍然很慢

ResNet
![图 21](images/592711ce992978289c28d35bf1083e0f6ee31191083f544ac5e6ec432fcdd6bc.png)  
> 很快
> 貌似有点过拟合
