# $L1L2$正则化

[toc]

**正则化**：凡是能减少泛化误差而不是减少训练误差的方法都可以称作正则化方法。即可以减少过拟合的方法。

正则化是减少机器学习过拟合的过程，正则化最常见的方法之一就是对模型的权重进行$L1和L2$正则化。

正则化一般用到的是$L1和L2$范数：
$L2范数$：$||W||_2 = \sqrt{|w_1|^2+|w_2|^2+...+|w_n|^2}$
$L1范数$：$||W||_1 = |w_1|+|w_2|+...+|w_n|$

![图 1](../images/6f261b8f4691767c3b1114475a29d10ea56e001b0b684fbdf297b9fe337d3aa8.png)  

> 由图可见$L1和L2$具备凸集的性质

---

我们训练神经网络的目的是使损失函数最小，不论$w和b$数值的大小。但是对于不同的$w和b$，可能得到相同的损失。
但是**我们在预测新样本时，神经网络中参数的值的大小就很关键了**。
比如当你的参数比较大时，**对于新样本中的误差和噪声，与大参数相乘后也会被放大**，那么就容易出问题。故要控制参数的取值不要那么大。
同时呢高次项的系数变小了，函数就变得平滑了。
> **减小参数大小近似于去掉了某些特征**，虽然没有真正去掉，但是接近于忽略。[参考](https://blog.csdn.net/qq_40639302/article/details/120323360)

==**那么如何控制参数不要太大？**==

> 由于$b$只影响怎么平移，不重要，即控制参数$w$即可。

## 拉格朗日乘数法角度

那么可以用**拉格朗日乘数法**来求参数范围受限时的最值：（给一个可行域，在可行域范围内求最值）

使用范数作为**硬性限制**：

$L1正则化$：
$$
min\ \ loss(W,b) \\
s.t.\ \ \  ||W||_1 - C\leq 0
$$

$L1$范数代表了$W$对应的那个点到原点的$L1$距离，使其$\leq C$，$C$越小意味着更强的正则项。

$L2正则化$：
$$
min\ \ loss(W,b) \\
s.t.\ \ \  ||W||_2 - C\leq 0
$$
![图 2](../images/a69b8198e52fe8733e6ff56341f98eeabbb18afa47778e478afacff7d9120ef0.png)  

使用范数作为**柔性限制**：
> 柔在何处？硬性限制中圆的半径$C$固定，柔性限制的半径$C$不固定，包含了任意半径的圆。[视屏](https://www.bilibili.com/video/BV1Z44y147xA?spm_id_from=333.999.0.0&vd_source=31f382886b368673a25ce3ff23e82bfc&t=1179.5)

下图中红绿问题取得最值时的参数$W$一样。二者问题等价。
![图 3](../images/296b575629e7273bc4eedb4ecef7e7645d351ef03d0bc8c208230f19aa84c6a6.png)  

由下图可见，$L1$正则化可带来**稀疏性**，稀疏性就是指在某些轴（$W$的某些项上）有值，而其他项是零。**即某些特征起作用，某些特征不起作用，即只通过某些特征就可以对新样本做出判断**。
> 下图可见，不同的$C$对应的最值点大多都在$y$轴上，$x$轴为0.

![图 4](../images/14a3de23ff654bb3a2a25c82008c0a099b793b1b4af1c3f0e102259d0972ed01.png)  

## 权重衰减角度

拉格朗日乘数法是控制权重$w$到原点的距离约束权重的取值。

权重衰退通过$L2$正则项每一次对$W$进行衰减，使得模型的参数不会过大，通过限制参数的选择范围来控制模型容量(复杂度)。

![图 1](../images/dbcaef3441a14635cfc32673c62528c323ff4b58e0c422d46b13b2e6aa2e21b6.png)  

> 上图损失函数的两个等式是等价关系

如上图，正则化后，每一次权重更新，每一步梯度下降法的学习过程，都让$W$进行一定的衰减。

**为什么增加惩罚项之后，就能避免过拟合？**
我们知道过拟合时，决策边界有很多个弯曲，而决策边界对应的函数中如果函数次数越高对应着越多的弯曲，故可通过减小高次项的系数来减少高次项对整个函数的贡献，减小高次项系数可以通过增加惩罚项来实现。
