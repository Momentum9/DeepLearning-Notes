{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现VGG块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels): # 卷积层数量、输入通道数量、输出通道数量\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels # 下一层的输入通道数量等于上一层的输出通道数量\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers) # 解包\n",
    "\n",
    "# 每个卷积核的通道数量和输入的通道数量一致\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG网络<br>\n",
    "前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义VGG架构 (卷积层数，通道数) 5个VGG块\n",
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n",
    "# 64代表输出通道的数量，等于卷积核的数量\n",
    "\n",
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1  # 每个卷积核的通道数量和输入的通道数量一致 我们是单色图吧\n",
    "    # 卷积层部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, nn.Flatten(),  # 输入到全连接层之前需要flatten\n",
    "        # 全连接层部分\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "每个block的高宽减半 通道数翻倍\n",
    "blk-1 (1, 64)\n",
    "Conv  (224+1*2-3+1) = 224\n",
    "MaxP  (224-2)/2+1 = 112\n",
    "64*112*112\n",
    "\n",
    "blk-2 (1, 128)\n",
    "Conv  (112+1*2-3+1) = 112\n",
    "MaxP  (112-2)/2+1 = 56\n",
    "128*56*56\n",
    "\n",
    "blk-3 (2, 256)\n",
    "Conv  (56+1*2-3+1) = 56\n",
    "Conv  (56+1*2-3+1) = 56 (卷积两次 就池化一次)\n",
    "MaxP  (56-2)/2+1 = 28\n",
    "256*28*28\n",
    "\n",
    "blk-4 (2,512)\n",
    "Conv  28\n",
    "Conv  28\n",
    "MaxP  14\n",
    "512*14*14\n",
    "\n",
    "blk-5 (2, 512)\n",
    "14\n",
    "14\n",
    "7\n",
    "512*7*7\n",
    "\n",
    "FC\n",
    "512*7*7 -> 4096\n",
    "4096 -> 4096\n",
    "4096 -> 10 # fashion-MNIST\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 构建一个高宽均为224的单通道数据样本，以观察每个层的输出形状\n",
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们此处让通道数减少4倍 计算量减少了16倍\n",
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52c95d02f1bfe7c59da35d3ff8fa76f7d162251a8bf0068369a0a87df4b3e5e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
