# BART

BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

Bidirectional and Auto-Regressive Transformers

==first==

BERT + GPT结合体
也即encoder + decoder

![picture 0](../images/b95ed32a6f9ea5cf682d5062c2a5858a718c833e48e885187cbdf84970973d66.png)  

BART是一个encoder-decoder的结构，其encoder端的输入是加了噪音的序列，decoder端的输入是right-shifted的序列，decoder端的目标是原序列。模型设计的目的很明确，就是在利用encoder端的双向建模能力的同时，保留自回归的特性，以适用于生成任务

==second==

相对于BERT中单一的noise类型(只有简单地用[MASK] token进行替换这一种noise)，BART在encoder端尝试了多种noise
![picture 2](../images/6e15d4b13c8f92cb7a18d5e99de5126fe60d3bc7dcdb8e732558dd3440c0bba6.png)  
