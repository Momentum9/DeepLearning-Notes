# Note

[toc]

GPT和BERT的区别还在于目标函数的选取，GPT预测未来，BERT预测中间（完形填空）
GPT系列的局限性，只能往前看，不能双向学习。
**GPT1**只使用Transformer的Decoder模块；**核心思想**：**无标签文本数据预训练+有标签数据微调**;

**BERT**只使用Transformer的Encoder模块，BERT Base比GPT1好(1.2亿参数)，然后还训练了一个BERT Large(3.4亿参数)

**GPT2**技术路线不变，依然用Decoder模块，然后收集了比BERT更大的数据集，训练了更大的模型(15亿参数)，比BERT Large大，然后提出了**zero-shot**设定（在做下游任务时，不需要下有任务的任何标注的信息，即**只用无标签数据进行预训练，让模型自己学会解决多任务问题**）(*新意度拉得很高，但有效性不足*)

**GPT3**数据和模型都比GPT2大了100倍（1750亿个参数）；**不进行梯度更新或者微调，仅使用个别示例与模型进行文本交互；**暴力出奇迹，效果非常惊艳；
（*补充有效性，降低新意度，将zero-shot改为few-shot*）

## GPT1

**问题背景：**
无标签的文本很多，但是针对特定任务的有标签文本数据很稀少，导致针对特定任务来训练一个准确的模型很有挑战。

**解决方法：**
在无标签数据上训练一个语言模型，在下游具体任务上用有标签数据进行微调。

> GPT的''预训练-微调''思想其实八九年在cv领域已经很火热，但由于NLP领域没有类似于ImageNet这种大规模标记好的数据。但GPT中使用的是**没有标号的数据**进行预训练，然后迁移到各种有监督的NLP任务上，并对参数进行fine-tuning。

**核心思想**
**无监督预训练 + 有监督微调**

预训练：
使用decoder中的masked自注意力保证在预测第t个词时只看到t时刻前面的k个词语（k是上下文窗口）

GPT的这个目标函数是更难的，预测未来肯定比完形填空（BERT）更难，所以GPT1效果要弱于BERT，但作者不断推进，搞到GPT3之后功能相当炸裂（天花板高于BERT了）

**不同下游任务的输入**
增加起始符、结束符和分隔符三种特殊符号的向量参数
![图 4](../../images/af7a3f313d8e5acd97872e0b3a97a9f2dc6985b0e25c992f44e73fad77bfc116.png)  

## GPT2

**问题背景：**
使用预训练+微调的方式虽然能解决带标签文本稀少的问题，但是在针对具体任务时，仍然是需要重新训练，泛化性比较差，不能广泛应用。

**解决方法：**
与GPT-1不同，GPT-2彻底放弃微调阶段，仅通过大规模多领域的数据预训练，让模型在Zero-shot Learning的设置下自己学会解决多任务的问题语言模型.

**核心思想：**

1. 引入zero-shot设定，去掉微调过程；在执行下游任务时，不需要任何标记样本，也不需要针对下游任务进行微调建模。只要训练好一个模型，在任何地方都可以用，实现从已知领域到未知领域的迁移学习。
2. 增加数据集和网络参数（15亿）

我们知道GPT1中对于不同下游任务的输入进行了不同的构造（开始符、中间符和结束符），模型不认识这些符号，但是微调的时候，给定一些labeled训练样本使模型可以认识这些符号的含义。
但此时，要做zero-shot（做下有任务的时候模型不能被调整），所以不能在下游任务的输入引入一些模型不认识的符号。所以，此处用了“特殊分割符-prompt提示”（2018年McCann的工作）
![图 5](../../images/86ae9c7bf28b1c70c8003f7f2cc8599bb5fbffdc063de26213c95098be978d85.png)  

巨大模型加上全程无监督学习zero shot确实使有一些任务表现很好，比如具备很强的文本生成能力，但有一些任务表现不咋地。但实验表明随着模型量和数据量的增加，表现在变得更好。

## GPT3

**问题背景：**

1. 如果下游子任务不用标注数据进行微调很难取得不错的效果（GPT2）。
2. 虽然微调效果很好，但标注数据的成本又是很高的，而且会导致模型学到一些虚假的特征，造成过拟合，使模型泛化性能变差。

**解决方法：**
与GPT-1，GPT-2不同，GPT3在**下游任务应用时不进行梯度更新或者微调，仅使用任务说明和个别示例与模型进行文本交互**，并且使用更大的模型，大力出奇迹。

**细节：**
GPT提出了一种in-context learning的方法，就是给出任务的描述和一些参考案例的情况下，模型能根据当前任务描述、参数案例明白到当前的语境。GPT并没有利用实例进行Fine-tune，而是让案例作为一种输入的指导，帮助模型更好的完成任务。

引入了：
few-shot
在inference time，只给模型某个特定任务的说明和**一些示例**，但不进行权重更新。
one-shot
在inference time，只给模型某个特定任务的说明和**一个示例**，不进行权重更新
zero-shot
在inference time，只给模型某个特定任务的说明，**不给示例**，不进行权重更新

最后，我们发现 GPT-3 可以生成人类评估者难以将其与人类撰写的文章区分开来的新闻文章样本。

**Related：**

1. [李沐](https://www.bilibili.com/video/BV1AF411b7xQ/?spm_id_from=333.999.0.0)
2. [CSDN](https://blog.csdn.net/qq_42740834/article/details/125189405?ops_request_misc=&request_id=&biz_id=102&utm_term=gpt-123%E5%8C%BA%E5%88%AB&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-125189405.142^v59^js_top,201^v3^control_2&spm=1018.2226.3001.4187)
