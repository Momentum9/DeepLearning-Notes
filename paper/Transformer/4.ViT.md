# Note

ViT（vision transformer）是Google在2020年提出的直接将transformer应用在**图像分类**的模型，后面很多的工作都是基于ViT进行改进的。
ViT的思路很简单：直接**把图像分成固定大小的patchs，然后通过线性变换得到patch embedding**，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。
ViT模型原理如下图所示，其实ViT模型**只是用了transformer的Encoder来提取特征**（原始的transformer还有decoder部分，用于实现sequence to sequence，比如机器翻译）。
![图 1](../../images/1e8cd0fe77cad71a1d092bf18b2a7fafd5de4bec54d769e93b7cc39328602006.png)  

## 讲解

### 优缺点

优点：

- 推翻了CNN在CV届的统治地位
- 打破了CV和NLP之间的壁垒，为多模态的发展奠定了基础

### Embedding层

**Patch Embedding**
对于ViT来说，首先要将原始的2-D图像转换成一系列1-D的patch embeddings。
对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。

首先将一张图片按给定大小分成一堆Patches。以ViT-B/16为例，将输入图片(224x224)按照16x16大小的Patch进行划分，划分后会得到( 224 / 16)^2 = 196个Patches。接着**通过线性映射将每个Patch映射到一维向量中**，以ViT-B/16为例，每个Patche数据**shape为[16, 16, 3]**通过映射得到一个长度为768的向量（后面都直接称为token）。[16, 16, 3] -> [768]。

在**代码实现中，直接通过一个卷积层来实现**。 以ViT-B/16为例，直接使用一个卷积核大小为16x16，步距为16，卷积核个数为768的卷积来实现。通过卷积[224, 224, 3] -> [14, 14, 768]，然后把H以及W两个维度展平即可[14, 14, 768] -> [196, 768]，此时正好变成了一个二维矩阵，正是Transformer想要的。（196个token，每个token长768）

**class token**
在原论文中，作者说参考BERT，在刚刚得到的一堆tokens中插入一个专门用于分类的[class]token，这个[class]token是一个可训练的参数，数据格式和其他token一样都是一个向量，以ViT-B/16为例，就是一个长度为768的向量，与之前从图片中生成的tokens拼接在一起，Cat([1, 768], [196, 768]) -> [**197**, 768]。

**Position Embedding**
transformer和CNN不同，**需要position embedding来编码tokens的位置信息**，这主要是因为self-attention是permutation-invariant，即**打乱sequence里的tokens的顺序并不会改变结果**。**如果不给模型提供patch的位置信息，那么模型就需要通过patchs的语义来学习拼图，这就额外增加了学习成本**。
ViT论文中对比了几种不同的position embedding方案(如下），最后发现如果不提供positional embedding效果会差，但其它各种类型的positional embedding效果都接近，这主要是因为ViT的输入是相对较大的patchs而不是pixels，所以学习位置信息相对容易很多。
这里的Position Embedding采用的是一个可训练的参数（1D Pos. Emb.），是直接叠加在tokens上的（add），所以shape要一样。以ViT-B/16为例，刚刚拼接[class]token后shape是[197, 768]，那么这里的Position Embedding的**shape也是[197, 768]**。

### Transformer Encoder

多头注意力机制（MSA）的计算量是和$N^2$成正相关的，所以ViT的输入是patch embeddings，而不是pixel embeddings，这有计算量上的考虑。MSA之后接一个FFN，包括两个FC层，第一层将输入节点个数翻4倍[197, 768] -> [197, 3072]，第二个全连接层会还原回原节点个数。

### MLP Head

上面通过Transformer Encoder后输出的shape和输入的shape是保持不变的，以ViT-B/16为例，输入的是[197, 768]输出的还是[197, 768]。这里我们只是需要分类的信息，所以我们只需要提取出[class]token生成的对应结果就行，即[197, 768]中抽取出[class]token对应的[1, 768]。接着我们通过MLP Head得到我们最终的分类结果。MLP Head原论文中说在训练ImageNet21K时是由Linear+tanh激活函数+Linear组成。但是迁移到ImageNet1K上或者你自己的数据上时，只用一个Linear即可。

![图 2](../../images/a9feb1ed0820f75d6afc2e2c8169237896f679a88ee1fd59d66b6dbd00f2b235.png)  

### 完整架构

![图 3](../../images/dadf731edaf60dd73bed87948009d0c40914cbc840a7bce051ba0fbf40075166.png)  

参考：
[CSDN](https://blog.csdn.net/qq_37541097/article/details/118242600?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166623327316782425119295%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166623327316782425119295&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-118242600-null-null.142^v59^js_top,201^v3^control_2&utm_term=ViT&spm=1018.2226.3001.4187)