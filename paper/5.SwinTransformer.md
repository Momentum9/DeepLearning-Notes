# Note

[toc]

## 解决了什么问题

ViT在视觉领域只能用于分类，而SwinTransformer证明了transformer可作为视觉领域的通用骨干网络（包括分类、检测、分割和视屏等领域）

ViT将图片打成16*16的patch，每个patch自始至终代表的尺寸一样

> 对于检测任务，**多尺度的特征**是非常重要的，比如FPN将多个卷积层出来的特征的感受野是不一样的，能抓住物体不用尺度的特征，能很好地处理物体不同尺度的问题。
> 对于分割任务，UNet对于处理物体不同尺寸的问题，提出了skip connection的方法。

ViT只能用于单一尺寸，不适合处理密集预测型的任务。此外ViT的自注意力始终在整张图上进行，是全局建模，其复杂度是图像的平方倍，复杂度太高。

SwinTransformer在小窗口内计算自注意力，只需保证小窗口大小固定（该窗口内自注意力计算复杂度固定），那复杂度和图像的大小成线性关系。（利用了局部性理论：语义相近的不同物体大概率还是出现在相连的地方）

ST中的Patch Merging类似于池化层，使感受野增大，抓住了多尺度特征

ST在连续的自注意力层之间Shift Windows，使m层中某窗口中的像素在m+1层可能移动到另一个窗口，这样就可以和其他像素进行互动，拥有了更大的感受野

motivation
直接把transformer移至到CV中面临的挑战：

- 尺度不一致的问题
- 图像分辨率太大

## 框架

![图 4](../images/a909292cdefadbdbc72e75ab15ecd4cbecd3e424e0f159e06619bd08c6edb1c6.png)  

四个stage，每个stage之后图片尺寸减半，channel翻倍。

![图 5](../images/601be9e11de211cc6302383bc60c83c3cc08593aa30269fd8f3ac43e6b67079c.png)

![图 1](../images/26ac35d1202d5eb31fd6a3ea2046ea3a83a00fefaae2d7809481259b10f3761f.png)  

**patch partition**之后图像 缩减为1/4，通道变为48，然后使用Linear embedding层对输入特征矩阵的channel进行调整为C(Swin-T=96)，此处Linear embedding层中包括一个Layer Norm，调整channel之后对每个channel进行了layer norm的处理。

==使用卷积核大小为4*4，采用48个卷积核，stride=4，便实现了patch partition + Linear embedding==

![图 6](../images/66c54ecb8cf0ee41ca0c9b0338b8397ed4e1813def83955a86883cfc4a4b7a3d.png)

**patch merging** 的作用的下采样，下采样之后高宽减半，channel翻倍；方法：假设特征矩阵高宽4*4，以2x2大小为一个窗口，将窗口中相同位置上的像素取出，得到四个矩阵，然后在深度方向上拼接，然后在channel方向上进行layer norm的处理，然后使用全连接层(1x1卷积)将channel数减半。
![图 1](../images/d0c3e817509740de68877f204caca1d4c02e59d290cb8d2355e5129c4773ca16.png)  


![图 7](../images/8966b879345b57718f9ff017e4ed3845a72a3c22dec17f9dd552eaeb948004e1.png)

## 问题

### 如何生成多尺度特征？patch merging的意义是什么？

CNN的多尺度来源于池化操作。池化能够增大每一个卷积核能看到的感受野，使得每次池化后的特征抓住物体的不同尺寸。所以ST提出了类似的操作，patch merging，将相邻的小patch合成一个大patch，这样大patch就能够看到前面四个小patch看到的内容，感受野就增大了，同时抓住了多尺度的特征。将这种多尺度的特征图扔给FPN就可以做检测了，扔给UNet就可以分割了。

### 为什么要在窗口中做自注意力而非全局？

因为全局自注意力计算会导致与tokens成平方倍的计算复杂度，这不适于处理很多下游任务，尤其是密集预测型的任务（图像分割），或者是非常大尺寸的图片的时候，全局自注意力是非常贵的的。
同一个物体的不同部位或者语义相近的不同物体，还是大概率会出现在相邻的地方，所以即使在一个小范围的local中计算自注意力也是够用的，全局计算自注意力对于CV任务来说是有些浪费资源的。

### 为什么利用了locality这个inductive bias计算复杂度就从N2变成N了？

假设每个图片有$h\times w$个patches
$$
\Omega(MSA) = 4hwC^2+2(hw)^2C
$$
> 标准多头自注意力机制
> 公式由来见[视频](https://www.bilibili.com/video/BV13L4y1475U?t=2071.7)
> 注意：A(nxm) * B(mxn)的复杂度为O(n x m x n) [矩阵乘法复杂度详见](https://blog.csdn.net/qq_39463175/article/details/111818717?ops_request_misc=&request_id=&biz_id=102&utm_term=%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-111818717.142^v63^control,201^v3^control,213^v2^t3_control1&spm=1018.2226.3001.4187)
> 按照第一层特征图作为示例，56x56x96, h=w=56, Channel=96
> $\Omega(MSA)=4\times 56\times 56\times 96^2 + 2(56\times56)^2\times96\approx 116M+1888M=2004M$

$$
\Omega(W\text{-}MSA)=4hwC^2+2M^2hwC
$$
> 局部的多头自注意力机制
> M=7, 表示窗口中的patch大小
> $\Omega(MSA)=4\times 56\times 56\times 96^2 + 2\times7^2\times56 \times56 \times 96\approx 116M+29M=145M$
> **可见从(hw)^2变为了hw，从N2变为了N**

### 为何要做shifted window？以及如何做shifted window？

因为基于窗口的self-attention虽然解决了内存和计算量的问题，但窗口和窗口之间无法通信了，就达不到全局建模了，会限制模型的能力。所以得想法方法让窗口和窗口之间互相通信起来。我们先做基于窗口之间的self-attention（W-MSA），然后再做基于移动窗口的self-attention（SW-MSA）。

### 我们shifted window之后，四个窗口变成了九个窗口，计算复杂度大大增加怎么办？而且shifted之后的窗口大小不一，无法将其打包到一个batch进行计算啊？

shifted window之后，窗口数量从$\lceil{\dfrac{h}{M}}\rceil \times \lceil \dfrac{w}{M}\rceil$变为了$(\lceil{\dfrac{h}{M}}\rceil + 1) \times (\lceil \dfrac{w}{M}\rceil+1)$

作者提出的**高效的批次计算移动后窗口自注意力的方式，通过cyclic shift将9个窗口变为4，然后借助mask机制合理地计算自注意力**：
![图 1](../images/9d99b6bc7ceb1eb16411cc80fbd0faf20ec1fac95bc592749da1e081c2c94736.png)  

### mask如何实现的？

![图 2](../images/18e9dc14c1ceced0c0fdeb66e6f309bbe79c825b6da386739952a240c401a034.png)  

比如这张shifted之后的图，我们不希望块36之间计算自注意力，由于向右下移动$(\lfloor\frac{M}{2}\rfloor, \lfloor\frac{M}{2}\rfloor)$，此处移动了3格，所以块2共4行，块6共3行，分别flatten为28，21个向量，然后与其转置计算注意力，得到的矩阵**36和63都是不想要的，33和66是想要的**，故使用绿色的掩码模板。将矩阵和模板相加，很小的数softmax之后变为0。

四个窗口详见下图：
![图 3](../images/392d700a2c0831d9d088ad2afb701ff7a22f76c63de15e8ef41c9bdafc330d72.png)  

讲解详见[视屏](https://www.bilibili.com/video/BV13L4y1475U?t=2729.1)，巨妙！！
