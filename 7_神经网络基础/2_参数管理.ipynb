{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 访问参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2822],\n",
       "        [-0.2836]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "\n",
    "X = torch.rand(2, 4)\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.0858, -0.3450,  0.0214, -0.1154, -0.1853,  0.0702, -0.0239, -0.0585]])),\n",
       "             ('bias', tensor([-0.0056]))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0056], requires_grad=True)\n",
      "tensor([-0.0056])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias)) # Parameter定义的是一个可以优化的参数\n",
    "print(net[2].bias) # tensor 由data和grad组成\n",
    "print(net[2].bias.data)\n",
    "print(net[2].bias.grad) # 此时还未BP so梯度为None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])  # 星号大概是解包意味\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()]) # 这个网络的全部参数 relu层没有参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0952, -0.3883,  0.2296, -0.0054],\n",
       "                      [-0.0477,  0.4109,  0.0550,  0.2887],\n",
       "                      [ 0.3021,  0.3643, -0.1415, -0.4893],\n",
       "                      [ 0.3586,  0.1796,  0.2362,  0.4590],\n",
       "                      [ 0.3365, -0.0113, -0.2847,  0.1146],\n",
       "                      [ 0.1044,  0.1765, -0.3702, -0.3456],\n",
       "                      [-0.3012, -0.1206,  0.4690, -0.2217],\n",
       "                      [ 0.3657, -0.4289,  0.2269,  0.2933]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.0988,  0.2384, -0.1475, -0.0636, -0.3188,  0.3589, -0.1716,  0.0871])),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.0858, -0.3450,  0.0214, -0.1154, -0.1853,  0.0702, -0.0239, -0.0585]])),\n",
       "             ('2.bias', tensor([-0.0056]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.state_dict()['2.bias'] # 字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从嵌套块中收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1737],\n",
       "        [0.1738]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())  # (name, module) # \n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))  # (4,8)->(8,4)-> (4,8)->(8,4)-> (4,8)->(8,4)-> (4,8)->(8,4)-> (4,1)\n",
    "rgnet(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'即可以通过print(net)来了解网络长什么样子'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rgnet) \n",
    "\"\"\"即可以通过print(net)来了解网络长什么样子\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对net所有块做统一初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m): # 参数为Module\n",
    "    if type(m) == nn.Linear:  # 如果是全连接层的话\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)  # 初始化weight为正态分布\n",
    "        nn.init.zeros_(m.bias)  # 下划线代表替换函数 而非返回值类型\n",
    "        ## TODO init module里面包含了大量用来初始化的函数\n",
    "    \n",
    "net.apply(init_normal)  # 遍历整个net进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.0028,  0.0167,  0.0078,  0.0057],\n",
       "                      [ 0.0073,  0.0060,  0.0053, -0.0028],\n",
       "                      [-0.0226, -0.0012, -0.0012,  0.0010],\n",
       "                      [ 0.0134, -0.0065, -0.0015, -0.0014],\n",
       "                      [-0.0100, -0.0151,  0.0125,  0.0037],\n",
       "                      [-0.0003,  0.0188, -0.0049,  0.0106],\n",
       "                      [-0.0007, -0.0086,  0.0019,  0.0015],\n",
       "                      [-0.0014,  0.0056,  0.0127, -0.0124]])),\n",
       "             ('0.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0097,  0.0178, -0.0034, -0.0040,  0.0004, -0.0032, -0.0101, -0.0095]])),\n",
       "             ('2.bias', tensor([0.]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.]])),\n",
       "             ('0.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('2.weight', tensor([[1., 1., 1., 1., 1., 1., 1., 1.]])),\n",
       "             ('2.bias', tensor([0.]))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    \"\"\"比如说此处将其初始化为常数1  仅做展示  实际可别这么做\"\"\" \n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对net某些块应用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0998, -0.5786,  0.6705,  0.2441],\n",
       "                      [ 0.3831, -0.1285,  0.6515,  0.6388],\n",
       "                      [-0.0608,  0.0901,  0.2831,  0.1976],\n",
       "                      [ 0.7063, -0.3314, -0.3340, -0.6324],\n",
       "                      [-0.6237, -0.1408,  0.5775, -0.2012],\n",
       "                      [ 0.5034,  0.3039,  0.5971,  0.5655],\n",
       "                      [-0.2118,  0.6935,  0.6494,  0.3139],\n",
       "                      [ 0.0387,  0.1685,  0.3475,  0.0262]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.2149, -0.4341, -0.1474,  0.2054, -0.3340,  0.2776,  0.0420,  0.0963])),\n",
       "             ('2.weight', tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])),\n",
       "             ('2.bias', tensor([0.0940]))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "def xavier(m):  # 貌似是一个很知名的初始化方式 俺不清楚\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight) \n",
    "\n",
    "def init_42(m): \n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42) # 宇宙的答案 42\n",
    "\n",
    "net[0].apply(xavier) # 第一个全连接层用这个\n",
    "net[2].apply(init_42)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52c95d02f1bfe7c59da35d3ff8fa76f7d162251a8bf0068369a0a87df4b3e5e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
