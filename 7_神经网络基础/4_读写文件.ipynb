{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存储数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载和保存张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存张量\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')\n",
    "\n",
    "# 加载张量\n",
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存储一个张量列表，然后载入内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x,y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "x2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写入字典 (从字符串映射到张量的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x':x, 'y':y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存储模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载和保存模型参数<br>\n",
    "思考一下，整个模型不好存储，存下参数就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.1468,  0.1776,  0.1083,  ..., -0.1544, -0.0365,  0.0215],\n",
       "                      [ 0.1555,  0.0358, -0.0167,  ...,  0.0078, -0.1334,  0.0618],\n",
       "                      [-0.1813,  0.0030, -0.0446,  ...,  0.0290,  0.1827,  0.0977],\n",
       "                      ...,\n",
       "                      [-0.0624, -0.0120, -0.0701,  ..., -0.0054, -0.1180, -0.1159],\n",
       "                      [ 0.1780, -0.0015, -0.1902,  ...,  0.0556, -0.0849,  0.0781],\n",
       "                      [ 0.0743, -0.1456,  0.1431,  ...,  0.0568, -0.0824, -0.2091]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([-0.1470, -0.1126, -0.0338,  0.1389, -0.0379, -0.0129, -0.1170,  0.1143,\n",
       "                      -0.1976,  0.0881,  0.1911, -0.1842, -0.0071, -0.1714,  0.0930, -0.1521,\n",
       "                      -0.0197,  0.1058, -0.0522, -0.0329, -0.1956, -0.1331,  0.0983,  0.1527,\n",
       "                       0.1246,  0.1663, -0.1743,  0.0323, -0.1801, -0.1035, -0.1778, -0.0511,\n",
       "                      -0.1080, -0.2054, -0.0291, -0.1496,  0.2036, -0.0376,  0.0455,  0.1560,\n",
       "                      -0.0878, -0.0462, -0.1378, -0.1080, -0.0663,  0.2019, -0.1730, -0.0780,\n",
       "                      -0.0434, -0.2215, -0.2016, -0.1105, -0.1518,  0.1850, -0.0502,  0.1798,\n",
       "                      -0.0207,  0.1770, -0.1849, -0.1263,  0.2028,  0.1668,  0.1847,  0.1924,\n",
       "                      -0.1697, -0.1542, -0.1665,  0.1773, -0.1154,  0.1235, -0.0496, -0.1682,\n",
       "                       0.0114,  0.1517, -0.1354, -0.0200,  0.1952, -0.1607, -0.1359,  0.1325,\n",
       "                       0.0098, -0.0963,  0.1712,  0.1744,  0.1527, -0.1820, -0.0293, -0.0839,\n",
       "                      -0.1114,  0.0117, -0.2171,  0.1007,  0.0211,  0.0218,  0.1667, -0.1606,\n",
       "                      -0.0186,  0.0228,  0.1107, -0.1737, -0.1145,  0.0888, -0.0437,  0.0964,\n",
       "                      -0.1386,  0.0488, -0.0828, -0.0945,  0.0416,  0.0638,  0.0371,  0.2089,\n",
       "                       0.0268,  0.0092, -0.0520,  0.1981, -0.1127, -0.2132, -0.1114, -0.2201,\n",
       "                       0.1175, -0.1109, -0.2172,  0.1007,  0.1014,  0.0717,  0.0617, -0.0172,\n",
       "                      -0.0878, -0.0184, -0.2104,  0.0029,  0.0823, -0.0240, -0.1872,  0.1845,\n",
       "                      -0.1150,  0.1029, -0.0098, -0.0075,  0.0322, -0.1644, -0.1078, -0.1982,\n",
       "                       0.0287,  0.2012,  0.0035,  0.1358,  0.0047,  0.1593, -0.1999,  0.0506,\n",
       "                       0.0175,  0.0021,  0.2032,  0.1833,  0.1540, -0.0113,  0.2014, -0.2018,\n",
       "                       0.0725,  0.1047,  0.1209, -0.1005, -0.1462,  0.2021,  0.2236, -0.0100,\n",
       "                       0.0032,  0.0252,  0.0720,  0.0589, -0.1681,  0.0061, -0.1996, -0.0608,\n",
       "                       0.0692,  0.1001,  0.2208,  0.1739, -0.0282, -0.2194,  0.1810,  0.2130,\n",
       "                      -0.0859, -0.2084,  0.0185, -0.0810, -0.0516,  0.1248,  0.0201, -0.2189,\n",
       "                       0.0617,  0.1891,  0.0952,  0.0365, -0.1986,  0.1420, -0.0882, -0.1217,\n",
       "                       0.1104,  0.2209, -0.0580, -0.1606,  0.0301, -0.0370, -0.1260, -0.1325,\n",
       "                       0.0272,  0.1487,  0.0371, -0.1613, -0.1671, -0.0300, -0.1382, -0.2178,\n",
       "                      -0.1778, -0.0708,  0.0809,  0.1264,  0.2003, -0.1078,  0.1652, -0.1677,\n",
       "                       0.0171,  0.0382,  0.0303,  0.0880,  0.1459, -0.0287, -0.1401, -0.0656,\n",
       "                      -0.1193, -0.0152, -0.0089,  0.1782, -0.0186, -0.1381,  0.2233, -0.1702,\n",
       "                      -0.1445,  0.1413,  0.0671,  0.1734, -0.1766,  0.1334,  0.0498, -0.1968,\n",
       "                       0.1135, -0.1829, -0.0916,  0.0922,  0.2103,  0.2161, -0.1819, -0.0117])),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.0526,  0.0450, -0.0154,  ..., -0.0453,  0.0135,  0.0270],\n",
       "                      [ 0.0572, -0.0299, -0.0068,  ...,  0.0188,  0.0545, -0.0550],\n",
       "                      [-0.0132, -0.0382, -0.0104,  ..., -0.0039,  0.0204,  0.0449],\n",
       "                      ...,\n",
       "                      [ 0.0559,  0.0344, -0.0234,  ...,  0.0624, -0.0526,  0.0116],\n",
       "                      [ 0.0562, -0.0364,  0.0146,  ..., -0.0118,  0.0346, -0.0437],\n",
       "                      [ 0.0455,  0.0157, -0.0582,  ...,  0.0515, -0.0310,  0.0223]])),\n",
       "             ('output.bias',\n",
       "              tensor([-0.0160, -0.0085, -0.0594, -0.0509, -0.0561,  0.0260, -0.0252,  0.0404,\n",
       "                       0.0559,  0.0394]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn((2, 20))\n",
    "net(X)\n",
    "net.state_dict() # 我们这儿并没有反向传播更新参数 所以说此参数和模型初始化的参数一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')\n",
    "# state_dict()给出模型的所有参数信息，将其存入文件'mlp.params'中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何了load进来这个模型呢？<br>\n",
    "首先你需要将'mlp.params'文件和MLP模型文件存下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 0.1829, -0.1702, -0.1242,  ..., -0.2197,  0.1551, -0.1883],\n",
       "                      [ 0.1727, -0.1204, -0.0834,  ...,  0.1236, -0.1408,  0.1631],\n",
       "                      [-0.0018, -0.1559,  0.0014,  ..., -0.1992,  0.1859, -0.1096],\n",
       "                      ...,\n",
       "                      [ 0.0745,  0.1741, -0.1104,  ..., -0.0006,  0.0380, -0.1719],\n",
       "                      [ 0.2228, -0.2065,  0.0681,  ..., -0.1548,  0.1072, -0.1154],\n",
       "                      [ 0.1347,  0.0424, -0.0475,  ..., -0.0508,  0.0799,  0.2043]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([ 0.1032,  0.2030,  0.0548,  0.1337, -0.1278,  0.1162, -0.1547, -0.1803,\n",
       "                      -0.2155,  0.0194,  0.1343,  0.0308, -0.2006,  0.1551, -0.1674, -0.1454,\n",
       "                       0.1072, -0.1228, -0.1588,  0.1023, -0.0068,  0.1959,  0.0463,  0.1583,\n",
       "                      -0.2176, -0.1607,  0.0768, -0.1161,  0.0479, -0.0008, -0.0203,  0.1730,\n",
       "                       0.1072, -0.0446, -0.2142,  0.0519,  0.0827,  0.1705,  0.0342, -0.0064,\n",
       "                      -0.2008,  0.2031,  0.1767, -0.0021,  0.1913,  0.0991,  0.1826,  0.1654,\n",
       "                      -0.0459, -0.1963, -0.2223, -0.2205,  0.2162, -0.0907, -0.1544, -0.0535,\n",
       "                       0.2208,  0.0863, -0.2161,  0.0591, -0.0192, -0.0120,  0.0447,  0.1591,\n",
       "                      -0.1848,  0.1145,  0.1700, -0.0127, -0.0557, -0.1963, -0.0476, -0.0904,\n",
       "                      -0.1582,  0.0945, -0.0498, -0.2187, -0.0773, -0.0218, -0.2149,  0.1293,\n",
       "                       0.1048, -0.1966,  0.1789, -0.0484, -0.0422,  0.1171, -0.0970, -0.0449,\n",
       "                       0.0521,  0.1680,  0.1082,  0.0356, -0.0483, -0.1984, -0.1923, -0.1264,\n",
       "                       0.1406, -0.0842, -0.0197, -0.1578, -0.0866, -0.1491,  0.1891, -0.1383,\n",
       "                      -0.0644,  0.1356,  0.0799,  0.1227,  0.2049, -0.1934,  0.0162,  0.0717,\n",
       "                      -0.0968,  0.1362, -0.1572, -0.0368, -0.0498, -0.1581, -0.1021,  0.1155,\n",
       "                       0.1450,  0.0207, -0.0890, -0.1584,  0.0107, -0.0905,  0.1827, -0.0854,\n",
       "                       0.0555,  0.0121, -0.2163,  0.0758,  0.1968, -0.0561, -0.0576, -0.0827,\n",
       "                       0.1403, -0.0043, -0.0015, -0.0600,  0.2139, -0.1954, -0.0776,  0.1366,\n",
       "                      -0.0677, -0.0775, -0.1858,  0.1457,  0.0283, -0.0230,  0.1360, -0.0644,\n",
       "                       0.0948, -0.1572, -0.1652,  0.0833, -0.1255, -0.0312,  0.1792, -0.1167,\n",
       "                       0.1621, -0.2064,  0.0365, -0.0461,  0.0770, -0.0447, -0.0690,  0.0147,\n",
       "                      -0.0253, -0.0116,  0.2217, -0.0121,  0.0558,  0.0389, -0.1416, -0.0413,\n",
       "                       0.0853, -0.1501, -0.1361,  0.0291,  0.2065,  0.1957,  0.1847, -0.0961,\n",
       "                       0.2067, -0.2169,  0.0652,  0.2126,  0.0063, -0.0654,  0.0298, -0.1603,\n",
       "                       0.0750,  0.0720,  0.1690, -0.0764, -0.1111,  0.0759, -0.1893,  0.0419,\n",
       "                       0.0094, -0.1681, -0.0686, -0.1425,  0.0538,  0.2216, -0.1788,  0.0388,\n",
       "                      -0.2077, -0.1059,  0.1891,  0.2155, -0.2205, -0.0804,  0.2117,  0.1352,\n",
       "                       0.0819, -0.1990, -0.1111,  0.1109,  0.1374,  0.0577,  0.1645, -0.0459,\n",
       "                      -0.0302, -0.0556,  0.1625, -0.2059, -0.1918,  0.0566, -0.1712,  0.2184,\n",
       "                      -0.0794, -0.1768,  0.1282, -0.1888, -0.1369, -0.0946,  0.2098, -0.1110,\n",
       "                       0.1490,  0.0026, -0.2176,  0.0513, -0.1938, -0.0472,  0.0802,  0.1621,\n",
       "                       0.0599, -0.1803, -0.0393, -0.1306,  0.1045, -0.1024,  0.1274, -0.1592])),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.0572,  0.0421,  0.0292,  ..., -0.0501,  0.0330, -0.0040],\n",
       "                      [-0.0537,  0.0465, -0.0571,  ..., -0.0173,  0.0395, -0.0119],\n",
       "                      [ 0.0365, -0.0603, -0.0490,  ..., -0.0165,  0.0450, -0.0115],\n",
       "                      ...,\n",
       "                      [-0.0319,  0.0604,  0.0477,  ..., -0.0251, -0.0401,  0.0125],\n",
       "                      [-0.0318,  0.0524, -0.0213,  ..., -0.0588, -0.0435,  0.0164],\n",
       "                      [ 0.0307, -0.0512, -0.0581,  ...,  0.0357,  0.0032, -0.0443]])),\n",
       "             ('output.bias',\n",
       "              tensor([-0.0557, -0.0548,  0.0443, -0.0127, -0.0575, -0.0279,  0.0259,  0.0539,\n",
       "                      -0.0132,  0.0354]))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP() # 首先创建一个新的初始化了的MLP模型\n",
    "clone.state_dict() # 可见此处参数和上述不一致，是因为模型参数大概是随机初始化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.1468,  0.1776,  0.1083,  ..., -0.1544, -0.0365,  0.0215],\n",
       "                      [ 0.1555,  0.0358, -0.0167,  ...,  0.0078, -0.1334,  0.0618],\n",
       "                      [-0.1813,  0.0030, -0.0446,  ...,  0.0290,  0.1827,  0.0977],\n",
       "                      ...,\n",
       "                      [-0.0624, -0.0120, -0.0701,  ..., -0.0054, -0.1180, -0.1159],\n",
       "                      [ 0.1780, -0.0015, -0.1902,  ...,  0.0556, -0.0849,  0.0781],\n",
       "                      [ 0.0743, -0.1456,  0.1431,  ...,  0.0568, -0.0824, -0.2091]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([-0.1470, -0.1126, -0.0338,  0.1389, -0.0379, -0.0129, -0.1170,  0.1143,\n",
       "                      -0.1976,  0.0881,  0.1911, -0.1842, -0.0071, -0.1714,  0.0930, -0.1521,\n",
       "                      -0.0197,  0.1058, -0.0522, -0.0329, -0.1956, -0.1331,  0.0983,  0.1527,\n",
       "                       0.1246,  0.1663, -0.1743,  0.0323, -0.1801, -0.1035, -0.1778, -0.0511,\n",
       "                      -0.1080, -0.2054, -0.0291, -0.1496,  0.2036, -0.0376,  0.0455,  0.1560,\n",
       "                      -0.0878, -0.0462, -0.1378, -0.1080, -0.0663,  0.2019, -0.1730, -0.0780,\n",
       "                      -0.0434, -0.2215, -0.2016, -0.1105, -0.1518,  0.1850, -0.0502,  0.1798,\n",
       "                      -0.0207,  0.1770, -0.1849, -0.1263,  0.2028,  0.1668,  0.1847,  0.1924,\n",
       "                      -0.1697, -0.1542, -0.1665,  0.1773, -0.1154,  0.1235, -0.0496, -0.1682,\n",
       "                       0.0114,  0.1517, -0.1354, -0.0200,  0.1952, -0.1607, -0.1359,  0.1325,\n",
       "                       0.0098, -0.0963,  0.1712,  0.1744,  0.1527, -0.1820, -0.0293, -0.0839,\n",
       "                      -0.1114,  0.0117, -0.2171,  0.1007,  0.0211,  0.0218,  0.1667, -0.1606,\n",
       "                      -0.0186,  0.0228,  0.1107, -0.1737, -0.1145,  0.0888, -0.0437,  0.0964,\n",
       "                      -0.1386,  0.0488, -0.0828, -0.0945,  0.0416,  0.0638,  0.0371,  0.2089,\n",
       "                       0.0268,  0.0092, -0.0520,  0.1981, -0.1127, -0.2132, -0.1114, -0.2201,\n",
       "                       0.1175, -0.1109, -0.2172,  0.1007,  0.1014,  0.0717,  0.0617, -0.0172,\n",
       "                      -0.0878, -0.0184, -0.2104,  0.0029,  0.0823, -0.0240, -0.1872,  0.1845,\n",
       "                      -0.1150,  0.1029, -0.0098, -0.0075,  0.0322, -0.1644, -0.1078, -0.1982,\n",
       "                       0.0287,  0.2012,  0.0035,  0.1358,  0.0047,  0.1593, -0.1999,  0.0506,\n",
       "                       0.0175,  0.0021,  0.2032,  0.1833,  0.1540, -0.0113,  0.2014, -0.2018,\n",
       "                       0.0725,  0.1047,  0.1209, -0.1005, -0.1462,  0.2021,  0.2236, -0.0100,\n",
       "                       0.0032,  0.0252,  0.0720,  0.0589, -0.1681,  0.0061, -0.1996, -0.0608,\n",
       "                       0.0692,  0.1001,  0.2208,  0.1739, -0.0282, -0.2194,  0.1810,  0.2130,\n",
       "                      -0.0859, -0.2084,  0.0185, -0.0810, -0.0516,  0.1248,  0.0201, -0.2189,\n",
       "                       0.0617,  0.1891,  0.0952,  0.0365, -0.1986,  0.1420, -0.0882, -0.1217,\n",
       "                       0.1104,  0.2209, -0.0580, -0.1606,  0.0301, -0.0370, -0.1260, -0.1325,\n",
       "                       0.0272,  0.1487,  0.0371, -0.1613, -0.1671, -0.0300, -0.1382, -0.2178,\n",
       "                      -0.1778, -0.0708,  0.0809,  0.1264,  0.2003, -0.1078,  0.1652, -0.1677,\n",
       "                       0.0171,  0.0382,  0.0303,  0.0880,  0.1459, -0.0287, -0.1401, -0.0656,\n",
       "                      -0.1193, -0.0152, -0.0089,  0.1782, -0.0186, -0.1381,  0.2233, -0.1702,\n",
       "                      -0.1445,  0.1413,  0.0671,  0.1734, -0.1766,  0.1334,  0.0498, -0.1968,\n",
       "                       0.1135, -0.1829, -0.0916,  0.0922,  0.2103,  0.2161, -0.1819, -0.0117])),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.0526,  0.0450, -0.0154,  ..., -0.0453,  0.0135,  0.0270],\n",
       "                      [ 0.0572, -0.0299, -0.0068,  ...,  0.0188,  0.0545, -0.0550],\n",
       "                      [-0.0132, -0.0382, -0.0104,  ..., -0.0039,  0.0204,  0.0449],\n",
       "                      ...,\n",
       "                      [ 0.0559,  0.0344, -0.0234,  ...,  0.0624, -0.0526,  0.0116],\n",
       "                      [ 0.0562, -0.0364,  0.0146,  ..., -0.0118,  0.0346, -0.0437],\n",
       "                      [ 0.0455,  0.0157, -0.0582,  ...,  0.0515, -0.0310,  0.0223]])),\n",
       "             ('output.bias',\n",
       "              tensor([-0.0160, -0.0085, -0.0594, -0.0509, -0.0561,  0.0260, -0.0252,  0.0404,\n",
       "                       0.0559,  0.0394]))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone.load_state_dict(torch.load('mlp.params')) # 内层返回类型为 OrderedDict \n",
    "clone.state_dict() # 可见此时将上述模型参数load进来了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52c95d02f1bfe7c59da35d3ff8fa76f7d162251a8bf0068369a0a87df4b3e5e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
